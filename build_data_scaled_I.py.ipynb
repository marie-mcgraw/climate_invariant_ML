{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build training, validation, and testing hurricane data tensors.\n",
    "\n",
    "Functions\n",
    "---------\n",
    "build_data(data_path, settings, verbose=0)\n",
    "\n",
    "\"\"\"\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import toolbox\n",
    "\n",
    "__author__ = \"Elizabeth A. Barnes and Randal J Barnes\"\n",
    "__version__ = \"12 November 2022\"\n",
    "\n",
    "def build_data(data_path, settings, verbose=0, iscaled=False):\n",
    "    \"\"\"Build the training, validation and testing tensors for the shash model.\n",
    "\n",
    "    The settings['target'] specifies which data set to build.There are five\n",
    "    different possible targets: intensity, logitude, latitude, radial, and\n",
    "    angle.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data_path : str\n",
    "        The input filepath, not including the file name.\n",
    "\n",
    "    settings : dict\n",
    "        The parameters defining the current experiment.\n",
    "\n",
    "    verbose : int\n",
    "        0 -> silent\n",
    "        1 -> description only\n",
    "        2 -> description and y statistics\n",
    "        \n",
    "    ## Added by MCM\n",
    "    iscaled : boolean\n",
    "        True: calculate scaled intensity (I/MPI) and use that as preditor instead of VMAX\n",
    "        False (default): use VMAX as predictor (normal)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_summary : dict\n",
    "        A descriptive dictionary of the data.\n",
    "\n",
    "    x_train : numpy.ndarray\n",
    "        The training split of the x data.\n",
    "        shape = [n_train, n_features].\n",
    "\n",
    "    label_train : numpy.ndarray\n",
    "        The training split of the predictand.\n",
    "        shape = [n_train, 1].\n",
    "\n",
    "    x_val : numpy.ndarray\n",
    "        The validation split of the x data.\n",
    "        shape = [n_val, n_features].\n",
    "\n",
    "    label_val : numpy.ndarray\n",
    "        The validation split of the predictand.\n",
    "        shape = [n_val, 1].\n",
    "\n",
    "    x_test : numpy.ndarray\n",
    "        The test split of the x data.\n",
    "        shape = [n_test, n_features].\n",
    "\n",
    "    label_test : numpy.ndarray\n",
    "        The validation split of the predictand.\n",
    "        shape = [n_test, 1].\n",
    "\n",
    "    x_valtest : numpy.ndarray\n",
    "        The union of the test and validation splits of the x data.\n",
    "        shape = [n_val+n_test, n_features].\n",
    "\n",
    "    label_valtest : numpy.ndarray\n",
    "        The union of the test and validation splits of the predictand.\n",
    "        shape = [n_val+n_test, 1].\n",
    "\n",
    "    df_train : pandas dataframe\n",
    "        A pandas dataframe containing training records.  The\n",
    "        dataframe contains all columns from the original file.\n",
    "        However, the dataframe contains only rows from the training\n",
    "        data set that satisfy the specified basin and leadtime\n",
    "        requirements, and were not eliminated due to missing values.\n",
    "        The dataframe has the shuffled order of the rows.  In\n",
    "        particular, the rows of df_train align with the rows of x_train\n",
    "        and label_train.\n",
    "\n",
    "    df_val : pandas dataframe\n",
    "        A pandas dataframe containing validation records.  The\n",
    "        dataframe contains all columns from the original file.\n",
    "        However, the dataframe contains only rows from the validation\n",
    "        data set that satisfy the specified basin and leadtime\n",
    "        requirements, and were not eliminated due to missing values.\n",
    "        The dataframe has the shuffled order of the rows.  In particular,\n",
    "        the rows of df_val align with the rows of x_val and label_val.\n",
    "\n",
    "    df_test : pandas dataframe\n",
    "        A pandas dataframe containing testing records.  The\n",
    "        dataframe contains all columns from the original file.\n",
    "        However, the dataframe contains only rows from the testing\n",
    "        data set that satisfy the specified basin and leadtime\n",
    "        requirements, and were not eliminated due to missing values.\n",
    "        The dataframe has the shuffled order of the rows.  In particular,\n",
    "        the rows of df_test align with the rows of x_test and label_test.\n",
    "\n",
    "    df_valtest : pandas dataframe\n",
    "        A pandas dataframe containing union of the validation and testing\n",
    "        records.  The dataframe contains all columns from the original\n",
    "        file. However, the dataframe contains only rows from the union\n",
    "        of the validation and testing data sets that satisfy the specified\n",
    "        basin and leadtime requirements, and were not eliminated due to\n",
    "        missing values. The dataframe has the shuffled order of the rows.\n",
    "        In particular, the rows of df_valtest align with the rows of\n",
    "        x_valtest and label_valtest.\n",
    "\n",
    "    \"\"\"\n",
    "    # Setup for the selected features\n",
    "    if (settings[\"x_names\"] is None) && (iscaled == False):\n",
    "        x_names = [\n",
    "            \"VMAX0\",\n",
    "            \"VMXC\",\n",
    "            \"NCI\",\n",
    "            \"DSDV\",\n",
    "            \"LGDV\",\n",
    "            \"HWDV\",\n",
    "            \"AVDV\",\n",
    "            \"DV12\",\n",
    "            \"SLAT\",\n",
    "            \"SSTN\",\n",
    "            \"SHDC\",\n",
    "            \"DTL\",\n",
    "            \"T200\",\n",
    "            \"D200\",\n",
    "            \"RHMD\",\n",
    "            # \"SPDX\",\n",
    "            # \"SPDY\",\n",
    "        ]\n",
    "    elif (settings[\"x_names\"] is None) && (iscaled == True):\n",
    "         x_names = [\n",
    "            \"DTL\",\n",
    "            \"LAT\",\n",
    "            \"SHRG\",\n",
    "            \"D200\",\n",
    "            \"Z850\",\n",
    "            \"DELV\",\n",
    "            \"RHMD\",\n",
    "            \"DELV-12\",\n",
    "            \"SST\",\n",
    "            \"OHC\"]\n",
    "    else:\n",
    "        x_names = settings[\"x_names\"]\n",
    "\n",
    "    # Setup for the selected target.\n",
    "    if settings[\"target\"] == \"intensity\":\n",
    "        y_name = [\"PREDICTAND\"]\n",
    "        missing = -9999\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Get the data from the specified file and filter out the unwanted rows.\n",
    "    datafile_path = data_path + settings[\"filename\"]\n",
    "    df_raw = pd.read_table(datafile_path, sep=\"\\s+\")\n",
    "    df_raw = df_raw.rename(columns={'Date': 'year'})\n",
    "\n",
    "    # Add predictand.\n",
    "    df_raw[\"PREDICTAND\"] = df_raw[\"OFDV\"]   # OFDV = BestTrack - Official Forecast\n",
    "\n",
    "    # Get predictors and storms of interest.\n",
    "    df = df_raw[\n",
    "        (df_raw[\"ATCF\"].str.contains(settings[\"basin\"])) &\n",
    "        (df_raw[\"ftime(hr)\"] == settings[\"leadtime\"])\n",
    "    ]\n",
    "\n",
    "    # Replace missing values.\n",
    "    df = df.replace(missing, np.nan)\n",
    "    df = df.dropna(axis=0)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Shuffle the rows in the df Dataframe, using the numpy rng.\n",
    "    df = df.sample(frac=1, random_state=settings['rng_seed'])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Check that there is data for training.\n",
    "    if np.shape(df)[0] == 0:\n",
    "        return (\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "            np.empty((0, 1)),\n",
    "        )\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Train/Validation/Test Split\n",
    "\n",
    "    # Get the testing data\n",
    "    if settings[\"test_condition\"] is None:\n",
    "        pass\n",
    "    else:\n",
    "        years = settings[\"years_test\"]\n",
    "        if verbose != 0:\n",
    "            print('years' + str(years) + ' withheld for testing')\n",
    "        index = df.index[df['year'].isin(years)]\n",
    "        df_test = df.iloc[index]\n",
    "        x_test = df_test[x_names].to_numpy()\n",
    "        y_test = np.squeeze(df_test[y_name].to_numpy())\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "        df = df.drop(index)\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    # Get the validation data.\n",
    "    if settings[\"val_condition\"] == \"random\":\n",
    "        index = np.arange(0,settings[\"n_val\"])\n",
    "        if(len(index)<100):\n",
    "            raise Warning(\"Are you sure you want n_val < 100?\")\n",
    "\n",
    "    elif settings[\"val_condition\"] == \"years\":\n",
    "        if verbose != 0:\n",
    "            print('years' + str(settings[\"n_val\"]) + ' withheld for testing')\n",
    "        index = df.index[df['year'].isin(settings[\"n_val\"])]\n",
    "\n",
    "        # unique_years = df['year'].unique()\n",
    "        # years = unique_years[:settings[\"n_val\"]]\n",
    "        # index = df.index[df['year'].isin(years)]\n",
    "\n",
    "    df_val = df.iloc[index]\n",
    "    x_val = df_val[x_names].to_numpy()\n",
    "    y_val = np.squeeze(df_val[y_name].to_numpy())\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "    df = df.drop(index)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if settings[\"test_condition\"] is None:\n",
    "        df_test = df_val.copy()\n",
    "        x_test  = copy.deepcopy(x_val)\n",
    "        y_test  = copy.deepcopy(y_val)\n",
    "\n",
    "    # Subsample training if desired.\n",
    "    if settings[\"n_train\"] == \"max\":\n",
    "        df_train = df.copy()\n",
    "    else:\n",
    "        df_train = df.iloc[:settings[\"n_train\"]]\n",
    "\n",
    "    x_train = df_train[x_names].to_numpy()\n",
    "    y_train = np.squeeze(df_train[y_name].to_numpy())\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Create 'label' y arrays.\n",
    "\n",
    "    label_train = np.zeros((len(y_train), 1))\n",
    "    label_val = np.zeros((len(y_val), 1))\n",
    "    label_test = np.zeros((len(y_test), 1))\n",
    "\n",
    "    label_train[:, 0] = y_train\n",
    "    label_val[:, 0] = y_val\n",
    "    label_test[:, 0] = y_test\n",
    "\n",
    "    # Make a descriptive dictionary.\n",
    "    data_summary = {\n",
    "        \"datafile_path\": datafile_path,\n",
    "        \"x_train_shape\": tuple(x_train.shape),\n",
    "        \"x_val_shape\": tuple(x_val.shape),\n",
    "        \"x_test_shape\": tuple(x_test.shape),\n",
    "        \"label_train_shape\": tuple(label_train.shape),\n",
    "        \"label_val_shape\": tuple(label_val.shape),\n",
    "        \"label_test_shape\": tuple(label_test.shape),\n",
    "        \"x_names\": x_names,\n",
    "        \"y_name\": y_name,\n",
    "    }\n",
    "\n",
    "    # Report the results.\n",
    "    if verbose >= 1:\n",
    "        pprint.pprint(data_summary, width=80)\n",
    "\n",
    "    if verbose >= 2:\n",
    "        toolbox.print_summary_statistics(\n",
    "            {\n",
    "                \"y_train\" : label_train[:,0],\n",
    "                \"y_val\"   : label_val[:,0],\n",
    "                \"y_test\"  : label_test[:,0]\n",
    "            },\n",
    "            sigfigs=1\n",
    "        )\n",
    "\n",
    "    # Change dtype of label to 'float32' for consistency.\n",
    "    label_train = label_train.astype('float32')\n",
    "    label_val = label_val.astype('float32')\n",
    "    label_test = label_test.astype('float32')\n",
    "\n",
    "    # Create the combined valtest set.\n",
    "    x_valtest = np.concatenate((x_val, x_test), axis=0)\n",
    "    label_valtest = np.concatenate((label_val, label_test), axis=0)\n",
    "    df_valtest = pd.concat([df_val, df_test])\n",
    "\n",
    "    return (\n",
    "        data_summary,\n",
    "        x_train,\n",
    "        label_train,\n",
    "        x_val,\n",
    "        label_val,\n",
    "        x_test,\n",
    "        label_test,\n",
    "        x_valtest,\n",
    "        label_valtest,\n",
    "        df_train,\n",
    "        df_val,\n",
    "        df_test,\n",
    "        df_valtest,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHIPS",
   "language": "python",
   "name": "ships"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
